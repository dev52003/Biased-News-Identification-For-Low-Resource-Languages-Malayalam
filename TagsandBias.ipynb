{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "import logging\n",
        "\n",
        "# First configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Now we can use logger for GPU availability check\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    logger.info(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    logger.warning(\"Using CPU - GPU not available\")\n",
        "\n",
        "# Set recursion limit (safety measure)\n",
        "sys.setrecursionlimit(10000)\n",
        "\n",
        "# Initialize models with GPU optimization\n",
        "try:\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "\n",
        "    # Load models with device_map for automatic GPU placement\n",
        "    ner_model = pipeline(\n",
        "        \"ner\",\n",
        "        model=\"dslim/bert-base-NER\",\n",
        "        tokenizer=tokenizer,\n",
        "        aggregation_strategy=\"simple\",\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "        batch_size=8,  # Increased batch size for GPU\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32  # Use mixed precision on GPU\n",
        "    )\n",
        "\n",
        "    zero_shot_model = pipeline(\n",
        "        \"zero-shot-classification\",\n",
        "        model=\"facebook/bart-large-mnli\",\n",
        "        device=0 if torch.cuda.is_available() else -1,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "    )\n",
        "\n",
        "    # Explicitly move models to GPU if available\n",
        "    if torch.cuda.is_available():\n",
        "        ner_model.model = ner_model.model.to('cuda')\n",
        "        zero_shot_model.model = zero_shot_model.model.to('cuda')\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Model loading failed: {e}\")\n",
        "    raise\n",
        "\n",
        "# Optimized keyword fallback\n",
        "EVENT_FALLBACK = {\n",
        "    \"terrorism\": {\"attack\", \"terror\", \"bomb\", \"isis\"},\n",
        "    \"sports\": {\"match\", \"tournament\", \"score\", \"goal\"},\n",
        "    \"politics\": {\"election\", \"minister\", \"government\"},\n",
        "    \"accident\": {\"crash\", \"collision\", \"died\", \"killed\"}\n",
        "}\n",
        "\n",
        "def chunk_text(text, max_tokens=400):\n",
        "    \"\"\"Split text into safe chunks using tokenizer\"\"\"\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        yield tokenizer.convert_tokens_to_string(tokens[i:i+max_tokens])\n",
        "\n",
        "def extract_tags_safely(text):\n",
        "    \"\"\"Improved NER processing with proper token reconstruction\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s]', '', str(text))[:10000]  # Clean and truncate text\n",
        "    tags = defaultdict(list)\n",
        "\n",
        "    try:\n",
        "        # Process in chunks\n",
        "        for chunk in chunk_text(text):\n",
        "            if torch.cuda.is_available():\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    entities = ner_model(chunk)\n",
        "            else:\n",
        "                entities = ner_model(chunk)\n",
        "\n",
        "            # Variables to reconstruct split tokens\n",
        "            current_entity = None\n",
        "            reconstructed_text = \"\"\n",
        "\n",
        "            for entity in entities:\n",
        "                word = entity[\"word\"]\n",
        "\n",
        "                # Handle subword tokens (starting with ##)\n",
        "                if word.startswith(\"##\"):\n",
        "                    if current_entity:\n",
        "                        reconstructed_text += word[2:]  # Remove ## prefix\n",
        "                    continue\n",
        "\n",
        "                # If we have a reconstructed entity, save it\n",
        "                if current_entity and reconstructed_text:\n",
        "                    tags[current_entity[\"entity_group\"].lower() + \"s\"].append(reconstructed_text)\n",
        "\n",
        "                # Start new entity\n",
        "                current_entity = entity\n",
        "                reconstructed_text = word\n",
        "\n",
        "            # Add the last reconstructed entity if exists\n",
        "            if current_entity and reconstructed_text:\n",
        "                tags[current_entity[\"entity_group\"].lower() + \"s\"].append(reconstructed_text)\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"NER failed for text: {e}\")\n",
        "\n",
        "    # Post-processing to clean results\n",
        "    for key in tags:\n",
        "        # Remove single-letter entries and empty strings\n",
        "        tags[key] = [x for x in tags[key] if len(x) > 1 and x.strip()]\n",
        "\n",
        "        # Remove duplicates while preserving order\n",
        "        seen = set()\n",
        "        tags[key] = [x for x in tags[key] if not (x in seen or seen.add(x))]\n",
        "\n",
        "    return tags\n",
        "\n",
        "def detect_events(text):\n",
        "    \"\"\"Hybrid event detection with fallback\"\"\"\n",
        "    text_lower = text.lower()\n",
        "    events = []\n",
        "\n",
        "    # Try zero-shot first\n",
        "    try:\n",
        "        if torch.cuda.is_available():\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision for GPU\n",
        "                zs_result = zero_shot_model(\n",
        "                    text_lower[:1000],  # Truncate for zero-shot\n",
        "                    candidate_labels=list(EVENT_FALLBACK.keys()),\n",
        "                    multi_label=True\n",
        "                )\n",
        "        else:\n",
        "            zs_result = zero_shot_model(\n",
        "                text_lower[:1000],\n",
        "                candidate_labels=list(EVENT_FALLBACK.keys()),\n",
        "                multi_label=True\n",
        "            )\n",
        "\n",
        "        events.extend([\n",
        "            label for label, score in zip(zs_result[\"labels\"], zs_result[\"scores\"])\n",
        "            if score > 0.65\n",
        "        ])\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Zero-shot failed: {e}\")\n",
        "\n",
        "    # Keyword fallback if no events detected\n",
        "    if not events:\n",
        "        for event_type, keywords in EVENT_FALLBACK.items():\n",
        "            if any(kw in text_lower for kw in keywords):\n",
        "                events.append(event_type)\n",
        "\n",
        "    return events\n",
        "\n",
        "def process_row(row):\n",
        "    \"\"\"Safe row processing wrapper\"\"\"\n",
        "    try:\n",
        "        text = f\"{row['title_english']}. {row['summary_english']}\"\n",
        "        tags = extract_tags_safely(text)\n",
        "        tags[\"events\"] = detect_events(text)\n",
        "        return {k: list(set(v)) for k,v in tags.items()}\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed processing row: {e}\")\n",
        "        return {\"regions\": [], \"persons\": [], \"organizations\": [], \"events\": []}\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        df = pd.read_csv(\"/content/final_translated_news.csv\")\n",
        "        tqdm.pandas(desc=\"Tagging articles\")\n",
        "        df[\"tags\"] = df.progress_apply(process_row, axis=1)\n",
        "        df.to_csv(\"tagged_news_safe.csv\", index=False)\n",
        "        logger.info(\"Successfully processed %d articles\", len(df))\n",
        "    except Exception as e:\n",
        "        logger.critical(\"Fatal error: %s\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvtUOIL22-sv",
        "outputId": "d0083c9f-9d8b-4fff-e0a9-82a26350e071"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "Tagging articles:   0%|          | 0/225 [00:00<?, ?it/s]<ipython-input-3-be392982c4eb>:80: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "<ipython-input-3-be392982c4eb>:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision for GPU\n",
            "Tagging articles: 100%|██████████| 225/225 [00:26<00:00,  8.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas rapidfuzz pycountry geonamescache\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z3Kg2D1NXPL",
        "outputId": "d6ee5c64-17b6-42bd-b934-bb0ac2860e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting pycountry\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting geonamescache\n",
            "  Downloading geonamescache-2.0.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading geonamescache-2.0.0-py3-none-any.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: geonamescache, rapidfuzz, pycountry\n",
            "Successfully installed geonamescache-2.0.0 pycountry-24.6.1 rapidfuzz-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import time\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "\n",
        "# Load CSV\n",
        "df = pd.read_csv(\"tagged_news_safe.csv\")\n",
        "\n",
        "# Parse the string dict safely\n",
        "def safe_literal_eval(s):\n",
        "    try:\n",
        "        return ast.literal_eval(s)\n",
        "    except (ValueError, SyntaxError):\n",
        "        return {}\n",
        "\n",
        "df['tags'] = df['tags'].apply(safe_literal_eval)\n",
        "\n",
        "# Setup geocoder with timeout and error handling\n",
        "geolocator = Nominatim(user_agent=\"location_cleaner\", timeout=10)\n",
        "\n",
        "def safe_geocode(loc, max_retries=3):\n",
        "    for _ in range(max_retries):\n",
        "        try:\n",
        "            time.sleep(1.1)  # Respect Nominatim's 1 request per second policy\n",
        "            location = geolocator.geocode(loc, exactly_one=True, language='en')\n",
        "            if location:\n",
        "                # Get the last 3 components and strip whitespace\n",
        "                return \", \".join([part.strip() for part in location.address.split(\",\")[-3:]])\n",
        "            return None\n",
        "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "            print(f\"Geocoding error for '{loc}': {str(e)}\")\n",
        "            time.sleep(2)  # Wait longer if there's an error\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error for '{loc}': {str(e)}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_locs(locs_list):\n",
        "    if not isinstance(locs_list, list) or not locs_list:\n",
        "        return []\n",
        "\n",
        "    cleaned = []\n",
        "    for loc in locs_list:\n",
        "        if not isinstance(loc, str) or not loc.strip():\n",
        "            continue\n",
        "        cleaned_loc = safe_geocode(loc.strip())\n",
        "        if cleaned_loc and cleaned_loc not in cleaned:\n",
        "            cleaned.append(cleaned_loc)\n",
        "    return cleaned\n",
        "\n",
        "# Clean and update locs in tags\n",
        "for i, row in df.iterrows():\n",
        "    if not isinstance(row['tags'], dict):\n",
        "        continue\n",
        "\n",
        "    tags_dict = row['tags']\n",
        "    if 'locs' in tags_dict:\n",
        "        cleaned = clean_locs(tags_dict['locs'])\n",
        "        tags_dict['locs'] = cleaned\n",
        "    df.at[i, 'tags'] = tags_dict\n",
        "\n",
        "# Convert dicts back to string before saving\n",
        "df['tags'] = df['tags'].apply(str)\n",
        "\n",
        "# Save updated CSV\n",
        "df.to_csv(\"tagged_news_safe_updated.csv\", index=False)\n",
        "print(\"✅ 'locs' updated in 'tags' and saved to 'tagged_news_safe_updated.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63vtrE4_NUai",
        "outputId": "12a76b98-a0a2-4158-d1cb-48ad2ba7efdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 'locs' updated in 'tags' and saved to 'tagged_news_safe_updated.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gender-guesser"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz3mymwKJMCg",
        "outputId": "6dd45b6b-6f61-484d-fd1f-19f14119e2e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gender-guesser\n",
            "  Downloading gender_guesser-0.4.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Downloading gender_guesser-0.4.0-py2.py3-none-any.whl (379 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/379.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m266.2/379.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.3/379.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gender-guesser\n",
            "Successfully installed gender-guesser-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import gender_guesser.detector as gender\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/tagged_news_safe_updated.csv\")\n",
        "\n",
        "# Filter data for the specific agency (KerelaKaumudi) and create a COPY\n",
        "agency_name = \"KerelaKaumudi\"\n",
        "agency_df = df[df[\"MediaAgency\"] == agency_name].copy()\n",
        "\n",
        "# Initialize gender detector\n",
        "gender_detector = gender.Detector()\n",
        "\n",
        "# --- 1. Region Bias Score ---\n",
        "def extract_locations(tags_str):\n",
        "    if isinstance(tags_str, str) and \"'locs':\" in tags_str:\n",
        "        locs_part = tags_str.split(\"'locs':\")[1].split(\"]\")[0]\n",
        "        locs = [loc.strip(\" '\") for loc in locs_part.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\") if loc.strip(\" '\")]\n",
        "        return locs\n",
        "    return []\n",
        "\n",
        "agency_df[\"locations\"] = agency_df[\"tags\"].apply(extract_locations)\n",
        "all_locations = [loc for sublist in agency_df[\"locations\"] for loc in sublist]\n",
        "\n",
        "# Calculate region bias (entropy-based diversity score)\n",
        "unique_regions = list(set(all_locations))\n",
        "region_counts = {region: all_locations.count(region) for region in unique_regions}\n",
        "total_region_mentions = sum(region_counts.values())\n",
        "region_bias_score = 1 - max(region_counts.values()) / total_region_mentions if total_region_mentions > 0 else 0\n",
        "\n",
        "# --- Print Region Calculation Data ---\n",
        "print(\"\\n=== Region Bias Calculation ===\")\n",
        "print(f\"Total Locations Mentioned: {total_region_mentions}\")\n",
        "print(\"Breakdown by Region:\")\n",
        "for region, count in region_counts.items():\n",
        "    print(f\"- {region}: {count} mentions\")\n",
        "print(f\"Region Bias Score: {region_bias_score:.2f}\")\n",
        "\n",
        "# --- 2. Gender Bias Score (Using gender-guesser) ---\n",
        "def extract_gender_mentions(tags_str):\n",
        "    if isinstance(tags_str, str) and \"'pers':\" in tags_str:\n",
        "        pers_part = tags_str.split(\"'pers':\")[1].split(\"]\")[0]\n",
        "        persons = [p.strip(\" '\") for p in pers_part.replace(\"[\", \"\").replace(\"]\", \"\").split(\",\") if p.strip(\" '\")]\n",
        "        return persons\n",
        "    return []\n",
        "\n",
        "agency_df[\"persons\"] = agency_df[\"tags\"].apply(extract_gender_mentions)\n",
        "all_persons = [person for sublist in agency_df[\"persons\"] for person in sublist]\n",
        "\n",
        "# Estimate gender using gender-guesser\n",
        "male_mentions = 0\n",
        "female_mentions = 0\n",
        "unknown_gender = 0\n",
        "\n",
        "gender_data = []  # Store gender classification details\n",
        "\n",
        "for person in all_persons:\n",
        "    first_name = person.split()[0] if \" \" in person else person\n",
        "    gender_guess = gender_detector.get_gender(first_name)\n",
        "\n",
        "    if gender_guess in [\"male\", \"mostly_male\"]:\n",
        "        male_mentions += 1\n",
        "        gender_data.append((person, \"Male\"))\n",
        "    elif gender_guess in [\"female\", \"mostly_female\"]:\n",
        "        female_mentions += 1\n",
        "        gender_data.append((person, \"Female\"))\n",
        "    else:\n",
        "        unknown_gender += 1\n",
        "        gender_data.append((person, \"Unknown\"))\n",
        "\n",
        "total_gender_mentions = male_mentions + female_mentions\n",
        "gender_bias_score = abs(male_mentions - female_mentions) / total_gender_mentions if total_gender_mentions > 0 else 0\n",
        "\n",
        "# --- Print Gender Calculation Data ---\n",
        "print(\"\\n=== Gender Bias Calculation ===\")\n",
        "print(f\"Total Persons Mentioned: {len(all_persons)}\")\n",
        "print(f\"- Male: {male_mentions}\")\n",
        "print(f\"- Female: {female_mentions}\")\n",
        "print(f\"- Unknown/Unclassified: {unknown_gender}\")\n",
        "print(\"Gender Classification Details:\")\n",
        "for person, gender in gender_data:\n",
        "    print(f\"- {person}: {gender}\")\n",
        "print(f\"Gender Bias Score: {gender_bias_score:.2f}\")\n",
        "\n",
        "# --- 3. Demography Bias (Urban vs. Rural) ---\n",
        "urban_keywords = [\n",
        "    # Major Kerala Cities/Towns\n",
        "    \"Kochi\", \"Thiruvananthapuram\", \"Kozhikode\", \"Thrissur\", \"Kollam\",\n",
        "    \"Alappuzha\", \"Kannur\", \"Kottayam\", \"Palakkad\", \"Malappuram\",\n",
        "    \"Ernakulam\", \"Trivandrum\", \"Calicut\", \"Tellicherry\",\n",
        "\n",
        "    # Malayalam Urban Terms\n",
        "    \"nagaram\", \"purasabha\", \"mahanagaram\", \"town\", \"borough\",\n",
        "    \"corporation\", \"municipal area\",\n",
        "\n",
        "    # Commercial Hubs\n",
        "    \"business district\", \"commercial street\", \"CBD\", \"market area\",\n",
        "    \"chalai\", \"shopping complex\", \"high street\",\n",
        "\n",
        "    # Infrastructure\n",
        "    \"metro\", \"flyover\", \"mall\", \"apartment\", \"skyscraper\",\n",
        "    \"technopark\", \"infopark\", \"SEZ\", \"industrial estate\",\n",
        "\n",
        "    # Global Cities\n",
        "    \"Mumbai\", \"Delhi\", \"Bangalore\", \"Dubai\", \"Singapore\"\n",
        "]\n",
        "rural_keywords = [\n",
        "    # Kerala Village Terms\n",
        "    \"gramam\", \"ooru\", \"kudumbashree\", \"panchayat\", \"kudi\",\n",
        "    \"tharavadu\", \"kaavu\", \"paddy field\", \"kole lands\", \"kandal\",\n",
        "\n",
        "    # Geographic Features\n",
        "    \"kunnu\", \"puzha\", \"kadavu\", \"kayal\", \"kadu\", \"thodu\", \"padam\", \"nilam\",\n",
        "\n",
        "    # Rural Economy\n",
        "    \"karshaka\", \"krishi\", \"karshika\", \"thozhil\", \"pokkali\",\n",
        "    \"coir\", \"fishery\", \"toddy shop\", \"agrarian\",\n",
        "\n",
        "    # Cultural Terms\n",
        "    \"kettukazhcha\", \"pooram\", \"padayani\", \"theyyam\", \"vayanashala\",\n",
        "\n",
        "    # Generic Rural Terms\n",
        "    \"countryside\", \"hamlet\", \"remote\", \"tribal\", \"farmland\"\n",
        "]\n",
        "\n",
        "urban_mentions = sum(1 for loc in all_locations if any(keyword in loc for keyword in urban_keywords))\n",
        "rural_mentions = sum(1 for loc in all_locations if any(keyword in loc for keyword in rural_keywords))\n",
        "total_demo_mentions = urban_mentions + rural_mentions\n",
        "\n",
        "demography_bias_score = abs(urban_mentions - rural_mentions) / total_demo_mentions if total_demo_mentions > 0 else 0\n",
        "\n",
        "# --- Print Demography Calculation Data ---\n",
        "print(\"\\n=== Demography Bias Calculation ===\")\n",
        "print(f\"Total Location Mentions: {total_demo_mentions}\")\n",
        "print(f\"- Urban: {urban_mentions}\")\n",
        "print(f\"- Rural: {rural_mentions}\")\n",
        "print(f\"Demography Bias Score: {demography_bias_score:.2f}\")\n",
        "\n",
        "# --- Composite Coverage Bias Score ---\n",
        "coverage_bias_score = (region_bias_score + gender_bias_score + demography_bias_score) / 3\n",
        "\n",
        "# --- Final Results ---\n",
        "print(\"\\n=== Final Coverage Bias Score ===\")\n",
        "print(f\"Overall Coverage Bias Score: {coverage_bias_score:.2f}\")\n",
        "print(f\"- Region Bias: {region_bias_score:.2f}\")\n",
        "print(f\"- Gender Bias: {gender_bias_score:.2f}\")\n",
        "print(f\"- Demography Bias: {demography_bias_score:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZwl9o9VFcTv",
        "outputId": "ec784077-ac8f-4628-f5db-02e3698302a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Region Bias Calculation ===\n",
            "Total Locations Mentioned: 147\n",
            "Breakdown by Region:\n",
            "- 682035: 2 mentions\n",
            "- Ruvuma Region: 1 mentions\n",
            "- 192126: 1 mentions\n",
            "- Uttar Pradesh: 2 mentions\n",
            "- Kerala: 21 mentions\n",
            "- Pakistan: 4 mentions\n",
            "- 695521: 1 mentions\n",
            "- 682001: 1 mentions\n",
            "- Delhi: 5 mentions\n",
            "- 79601: 1 mentions\n",
            "- Northeast: 1 mentions\n",
            "- Alappuzha: 2 mentions\n",
            "- Rajasthan: 2 mentions\n",
            "- Goa: 1 mentions\n",
            "- 678102: 1 mentions\n",
            "- 678001: 1 mentions\n",
            "- 190001: 2 mentions\n",
            "- Bihar: 2 mentions\n",
            "- United Arab Emirates: 4 mentions\n",
            "- Estonia: 1 mentions\n",
            "- Canada: 1 mentions\n",
            "- 673001: 3 mentions\n",
            "- 47960: 1 mentions\n",
            "- Abu Dhabi Emirate: 2 mentions\n",
            "- India: 46 mentions\n",
            "- 110006: 4 mentions\n",
            "- 800001: 1 mentions\n",
            "- Southern Highlands Zone: 1 mentions\n",
            "- Tanzania: 1 mentions\n",
            "- 94000: 1 mentions\n",
            "- 190017: 1 mentions\n",
            "- Barmer: 2 mentions\n",
            "- Rapla County: 1 mentions\n",
            "- Abu Dhabi: 2 mentions\n",
            "- Czechia: 1 mentions\n",
            "- 695001: 7 mentions\n",
            "- Indiana: 1 mentions\n",
            "- 471 29: 1 mentions\n",
            "- Chandigarh: 1 mentions\n",
            "- United States: 1 mentions\n",
            "- 683101: 1 mentions\n",
            "- Metropolitan France: 1 mentions\n",
            "- Kanpur Nagar: 1 mentions\n",
            "- Jammu and Kashmir: 6 mentions\n",
            "- Punjab: 1 mentions\n",
            "- 671122: 1 mentions\n",
            "- France: 1 mentions\n",
            "Region Bias Score: 0.69\n",
            "\n",
            "=== Gender Bias Calculation ===\n",
            "Total Persons Mentioned: 71\n",
            "- Male: 10\n",
            "- Female: 6\n",
            "- Unknown/Unclassified: 55\n",
            "Gender Classification Details:\n",
            "- Sumuy Sure: Unknown\n",
            "- Mammootty: Unknown\n",
            "- Mohanlal: Unknown\n",
            "- Khab: Unknown\n",
            "- Sri: Unknown\n",
            "- Mohanlal: Unknown\n",
            "- KV Rabia: Unknown\n",
            "- Padma: Female\n",
            "- KV Raby: Unknown\n",
            "- Setu Lakshmi: Unknown\n",
            "- Avnat Ka: Unknown\n",
            "- Virat Kohl: Unknown\n",
            "- PakadK: Unknown\n",
            "- Ibrahim Badush: Male\n",
            "- Kallingal Mohammed Kullungal Mohammed Kutti: Unknown\n",
            "- Sadas: Unknown\n",
            "- Rajeev Chandrasekhar V: Male\n",
            "- Mohammed R: Male\n",
            "- Mohammed Riyaz: Male\n",
            "- Rajeev Chandrasekhand: Male\n",
            "- Ra: Unknown\n",
            "- Prasad: Male\n",
            "- Vishnu PrasadBeen: Unknown\n",
            "- Beena Antony: Female\n",
            "- Sreesanth: Unknown\n",
            "- Samson: Male\n",
            "- Kakka: Unknown\n",
            "- Lak: Unknown\n",
            "- Labo: Unknown\n",
            "- Ka: Unknown\n",
            "- Alappuzha: Unknown\n",
            "- Stephen Stephen: Male\n",
            "- Alleppey Ashraf: Unknown\n",
            "- Kamas: Unknown\n",
            "- Unni Mukunda: Female\n",
            "- Pinarayi Vijay: Unknown\n",
            "- KM Abraham: Unknown\n",
            "- Phu: Male\n",
            "- Sunju: Unknown\n",
            "- Sma: Unknown\n",
            "- Mattawa: Unknown\n",
            "- Nandana Varma: Unknown\n",
            "- Siwader: Unknown\n",
            "- Mom: Unknown\n",
            "- Kozhikode: Unknown\n",
            "- Sha: Unknown\n",
            "- Fatima: Female\n",
            "- Pi: Unknown\n",
            "- Pahal: Unknown\n",
            "- Malavi: Unknown\n",
            "- Mulaka Mohan: Unknown\n",
            "- Azarduddin Owy: Unknown\n",
            "- Modi: Unknown\n",
            "- Javan Muaresists: Unknown\n",
            "- Vineeta: Female\n",
            "- Rajendro: Unknown\n",
            "- Vinee: Unknown\n",
            "- Vligar Mukesh Nair: Unknown\n",
            "- Ki: Unknown\n",
            "- Saifa Khalid: Unknown\n",
            "- Big: Unknown\n",
            "- Dils: Unknown\n",
            "- Robin Radhakrishnan: Male\n",
            "- Narendra Modi: Unknown\n",
            "- Cyinna Mohammed Bin Salman: Unknown\n",
            "- Rajikumar: Unknown\n",
            "- Veena Vijay: Female\n",
            "- Pinarayi Vijay: Unknown\n",
            "- Idi: Unknown\n",
            "- Pocko Case: Unknown\n",
            "- Chacko Police: Unknown\n",
            "Gender Bias Score: 0.25\n",
            "\n",
            "=== Demography Bias Calculation ===\n",
            "Total Location Mentions: 9\n",
            "- Urban: 7\n",
            "- Rural: 2\n",
            "Demography Bias Score: 0.56\n",
            "\n",
            "=== Final Coverage Bias Score ===\n",
            "Overall Coverage Bias Score: 0.50\n",
            "- Region Bias: 0.69\n",
            "- Gender Bias: 0.25\n",
            "- Demography Bias: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaX3Fs1WHdbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}