{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3684b48-994a-46a0-aa9d-a7855e17eb16",
   "metadata": {},
   "source": [
    "## Mathrubhumi Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd0245b8-0929-4671-a9a5-30c49a204f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching recent sitemaps...\n",
      "Found 3 recent sitemaps, parsing articles...\n",
      "  Processing https://www.mathrubhumi.com/polopoly_fs/1.7292585.1746433801!/file.xml...\n",
      "  Processing https://www.mathrubhumi.com/polopoly_fs/1.10471980.1746036006!/file.xml...\n",
      "  Processing https://www.mathrubhumi.com/polopoly_fs/1.10552540.1746433802!/file.xml...\n",
      "\n",
      "Saved 75 articles to mathrubhumi_news_top_5_per_day_last_15_days_20250505.csv\n",
      "\n",
      "Sample of most recent articles:\n",
      "      date                                                                       title section                                                                                                                       url\n",
      "2025-05-05                 Two Youth Dies In Accident At Thiruvananthapuram 1.10562960  kerala                       https://www.mathrubhumi.com/news/kerala/two-youth-dies-in-accident-at-thiruvananthapuram-1.10562960\n",
      "2025-05-05 Listin Stephen Reacts On Controversies Nivin Pauli Sandra Thomas 1.10562952    news https://www.mathrubhumi.com/movies-music/news/listin-stephen-reacts-on-controversies-nivin-pauli-sandra-thomas-1.10562952\n",
      "2025-05-05 Man Arrested For Stabbing A Guest At A Wedding In Calicut Kerala 1.10562914    news        https://www.mathrubhumi.com/crime/news/man-arrested-for-stabbing-a-guest-at-a-wedding-in-calicut-kerala-1.10562914\n",
      "2025-05-05                          Defence Secretary Is Meeting Pm Sources 1.10562955   india                                 https://www.mathrubhumi.com/news/india/defence-secretary-is-meeting-pm-sources-1.10562955\n",
      "2025-05-05                        Jiostar Opposes Unified Tv Ott Regulation 1.10562957    news                          https://www.mathrubhumi.com/technology/news/jiostar-opposes-unified-tv-ott-regulation-1.10562957\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from urllib.parse import urlparse\n",
    "import re\n",
    "\n",
    "def get_recent_sitemaps(main_sitemap_url=\"https://www.mathrubhumi.com/sitemap.xml\", days_back=15):\n",
    "    \"\"\"Extract recent sitemaps from the main index\"\"\"\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(main_sitemap_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "        recent_sitemaps = []\n",
    "        \n",
    "        for sitemap in soup.find_all('sitemap'):\n",
    "            loc = sitemap.find('loc')\n",
    "            lastmod = sitemap.find('lastmod')\n",
    "            \n",
    "            if not loc or not loc.text:\n",
    "                continue\n",
    "                \n",
    "            # Check if sitemap is recent enough\n",
    "            sitemap_date = None\n",
    "            if lastmod and lastmod.text:\n",
    "                try:\n",
    "                    sitemap_date = datetime.strptime(lastmod.text[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "                except ValueError:\n",
    "                    pass\n",
    "            \n",
    "            # Also check for date patterns in the URL itself\n",
    "            url_date = None\n",
    "            for segment in loc.text.split('/'):\n",
    "                if segment.isdigit() and len(segment) == 8:  # YYYYMMDD format\n",
    "                    try:\n",
    "                        url_date = datetime.strptime(segment, \"%Y%m%d\")\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            # Use the most recent date we found\n",
    "            current_date = max(d for d in [sitemap_date, url_date] if d is not None)\n",
    "            \n",
    "            if current_date and current_date >= cutoff_date:\n",
    "                recent_sitemaps.append({\n",
    "                    'url': loc.text,\n",
    "                    'lastmod': current_date.strftime(\"%Y-%m-%d\")\n",
    "                })\n",
    "        \n",
    "        return recent_sitemaps\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching sitemap index: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def is_article_url(url):\n",
    "    \"\"\"Check if URL is an actual article and not a directory listing\"\"\"\n",
    "    # Check for common article URL patterns\n",
    "    patterns = [\n",
    "        r'-\\d+\\.\\d+$',  # Ends with -1.12345678\n",
    "        r'/\\d+$',       # Ends with /12345678\n",
    "        r'\\.html$',      # Ends with .html\n",
    "        r'\\.php\\?id=\\d+' # Ends with .php?id=123456\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        if re.search(pattern, url):\n",
    "            return True\n",
    "    \n",
    "    # Check if URL has more than 3 path segments (e.g., /news/kerala/article-1.123)\n",
    "    path_segments = urlparse(url).path.split('/')\n",
    "    if len([seg for seg in path_segments if seg]) > 3:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def parse_news_sitemap(sitemap_url, days_back=15):\n",
    "    \"\"\"Parse individual news sitemap and extract recent articles\"\"\"\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    articles = []\n",
    "    \n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        response = requests.get(sitemap_url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "        for url in soup.find_all('url'):\n",
    "            try:\n",
    "                loc = url.find('loc')\n",
    "                if not loc or not loc.text:\n",
    "                    continue\n",
    "                \n",
    "                url_str = loc.text.lower()\n",
    "                \n",
    "                # Check if URL is from /news/ directory and is an actual article\n",
    "                if '/news/' not in url_str or not is_article_url(url_str):\n",
    "                    continue\n",
    "                \n",
    "                # Extract and validate date\n",
    "                pub_date = None\n",
    "                lastmod = url.find('lastmod')\n",
    "                \n",
    "                if lastmod and lastmod.text:\n",
    "                    try:\n",
    "                        pub_date = datetime.strptime(lastmod.text[:10], \"%Y-%m-%d\")\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                \n",
    "                # Fallback to URL date parsing\n",
    "                if not pub_date:\n",
    "                    path = urlparse(loc.text).path\n",
    "                    for segment in path.split('/'):\n",
    "                        if segment.isdigit() and len(segment) == 8:  # YYYYMMDD\n",
    "                            try:\n",
    "                                pub_date = datetime.strptime(segment, \"%Y%m%d\")\n",
    "                                break\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "                \n",
    "                # Skip if no valid date found or too old\n",
    "                if not pub_date or pub_date < cutoff_date:\n",
    "                    continue\n",
    "                \n",
    "                # Extract title from URL if not in XML\n",
    "                title = url.find('news:title') or url.find('image:title')\n",
    "                title = title.text if title else loc.text.split('/')[-1].replace('-', ' ').title()\n",
    "                \n",
    "                # Extract section from URL path (second part of path after /news/)\n",
    "                path_parts = urlparse(loc.text).path.split('/')\n",
    "                section = path_parts[2] if len(path_parts) > 2 else 'general'\n",
    "                \n",
    "                articles.append({\n",
    "                    'title': title,\n",
    "                    'url': loc.text,\n",
    "                    'date': pub_date.strftime(\"%Y-%m-%d\"),\n",
    "                    'section': section,\n",
    "                    'datetime': pub_date  # Adding datetime for sorting\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing article in {sitemap_url}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing sitemap {sitemap_url}: {str(e)}\")\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def get_top_news_articles(days_back=15, articles_per_day=5):\n",
    "    \"\"\"Main function to get top news articles per day\"\"\"\n",
    "    print(\"Fetching recent sitemaps...\")\n",
    "    recent_sitemaps = get_recent_sitemaps(days_back=days_back)\n",
    "    \n",
    "    if not recent_sitemaps:\n",
    "        print(\"No recent sitemaps found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"Found {len(recent_sitemaps)} recent sitemaps, parsing articles...\")\n",
    "    all_articles = []\n",
    "    \n",
    "    for sitemap in recent_sitemaps:\n",
    "        print(f\"  Processing {sitemap['url']}...\")\n",
    "        articles = parse_news_sitemap(sitemap['url'], days_back=days_back)\n",
    "        all_articles.extend(articles)\n",
    "        time.sleep(1)  # Be polite\n",
    "    \n",
    "    if not all_articles:\n",
    "        print(\"No recent articles found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create DataFrame with strict date filtering\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Final date filter to ensure no old articles slip through\n",
    "    cutoff = datetime.now() - timedelta(days=days_back)\n",
    "    df = df[df['date'] >= cutoff]\n",
    "    \n",
    "    if df.empty:\n",
    "        print(f\"No articles within last {days_back} days after final filtering\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Clean and sort\n",
    "    df = df.drop_duplicates('url')\n",
    "    \n",
    "    # Get top articles per day\n",
    "    df = df.sort_values(['date', 'datetime'], ascending=False)\n",
    "    top_articles = df.groupby(df['date'].dt.date).head(articles_per_day)\n",
    "    \n",
    "    return top_articles.sort_values('date', ascending=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    days_back = 15\n",
    "    articles_per_day = 5\n",
    "    \n",
    "    news_df = get_top_news_articles(days_back=days_back, articles_per_day=articles_per_day)\n",
    "    \n",
    "    if not news_df.empty:\n",
    "        filename = f\"mathrubhumi_news_top_{articles_per_day}_per_day_last_{days_back}_days_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "        news_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSaved {len(news_df)} articles to {filename}\")\n",
    "        \n",
    "        print(\"\\nSample of most recent articles:\")\n",
    "        print(news_df[['date', 'title', 'section', 'url']].head().to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo recent articles found to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0019610-fbfd-47be-a3cc-d85dd1a1b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.11/site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.11/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: h11, attrs, wsproto, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-25.3.0 h11-0.16.0 outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986a1aa4-1947-4082-a9e7-2704a196715b",
   "metadata": {},
   "source": [
    "## Kerala Kaumudi Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bc0d0b-4b06-4ae6-b001-e275ee57fc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Kerala Kaumudi news...\n",
      "\n",
      "Saved 75 articles to kerala_kaumudi_top_5_per_day_last_15_days_20250505.csv\n",
      "\n",
      "Sample of most recent articles:\n",
      "      date                                                                                                                         title                                                                                                                                url\n",
      "2025-05-05         'മമ്മൂട്ടിയും മോഹൻലാലും ഒന്നും തന്നില്ലേയെന്നാണ് ചോദിക്കുന്നത്, സുബി സിനിമകൾ മനഃപൂർവം ഒഴിവാക്കി'; വെളിപ്പെടുത്തി അമ്മ                                              https://keralakaumudi.com/news/news.php?id=1528547&u=life-of-late-actress-subi-suresh\n",
      "2025-05-05                                                     പേവിഷബാധയേറ്റ് മരിച്ച നിയ ഫൈസലിന്റെ മൃതദേഹം ഖബറടക്കി; മാതാവ് ക്വാറന്റീനിൽ             https://keralakaumudi.com/news/news.php?id=1528494&u=body-of-nia-faisal-who-died-of-rabies-buried-mother-in-quarantine\n",
      "2025-05-05                  ആദ്യം ആര് റൊട്ടി കഴിക്കുമെന്ന് ചോദ്യം, വാക്കുതർക്കം കലാശിച്ചത് കൊലപാതകത്തിൽ; രണ്ട് കുട്ടികൾക്ക് ദാരുണാന്ത്യം             https://keralakaumudi.com/news/news.php?id=1528490&u=two-boys-killed-in-a-fight-over-food-during-a-wedding-celebration\n",
      "2025-05-05                                          ഭീകരരെ  സഹായിക്കുന്നവർക്കെതിരെ  ശക്തമായ  നടപടി; ജമ്മു കാശ്മീരിൽ 2800 പേർ കസ്റ്റഡിയിൽ       https://keralakaumudi.com/news/news.php?id=1528481&u=jammu-and-kashmir-police-strict-action-against-those-helping-terrorists\n",
      "2025-05-05                      ടൂറിസ്റ്റ് ബസിലും ഫോണിലും 'തുടരും'; 150 കടന്ന് വിജയകുതിപ്പ് നടത്തുന്ന ചിത്രത്തിന് ഭീഷണിയായി വ്യാജപതിപ്പ്                                            https://keralakaumudi.com/news/news.php?id=1528496&u=thudarum-movie-pirated-version-out\n",
      "2025-05-04                                                  തിളങ്ങുന്ന നീലക്കണ്ണുകളോടെ ജനിക്കുന്നത് ഭാഗ്യമാണോ?​ ഇവിടെയുളളവരുടേത് ദുരവസ്ഥ https://keralakaumudi.com/news/news.php?id=1527944&u=this-indonesian-tribe-has-sparkling-blue-eyes-but-this-beauty-brings-problems\n",
      "2025-05-04                  ഇനി കഷ്‌ടപ്പാടുകൾക്ക് വിട;  യുഎഇ ബിഗ് ടിക്കറ്റ് നറുക്കെടുപ്പിൽ വീണ്ടും മലയാളിക്ക് വിജയം, 57 കോടി രൂപ സ്വന്തം                        https://keralakaumudi.com/news/news.php?id=1527935&u=uae-big-ticket-draw-kerala-expat-winner-of-grand-prize\n",
      "2025-05-04                               പോളിയോയും അർബുദവും തളർത്തിയില്ല; സാക്ഷരതയുടെ വെളിച്ചം പകർന്ന പദ്‌മശ്രീ കെ  വി  റാബിയ അന്തരിച്ചു                                             https://keralakaumudi.com/news/news.php?id=1527931&u=padma-shri-k-v-rabiya-passed-away\n",
      "2025-05-04 'പലരും കറുമ്പിയെന്ന് വിളിച്ചു, സ്ത്രീധനം കിട്ടാതെ വന്നതോടെ ഭർത്താവ് പ്രശ്നമുണ്ടാക്കി; ഒരൊറ്റ കാരണം കൊണ്ട് ജീവിതം മാറിമറിഞ്ഞു'                            https://keralakaumudi.com/news/news.php?id=1527934&u=actress-sethu-lakshmi-share-the-tragic-family-life\n",
      "2025-05-04    ഫോട്ടോ  കണ്ടാൽ  കോൺഗ്രസുകാർക്ക്  പോലും  തിരിച്ചറിയാത്തവർ  വേണ്ട;  കെപിസിസി  നേതൃമാറ്റത്തിനെതിരെ  ആലുവയിൽ  വ്യാപക  പോസ്റ്റർ                                 https://keralakaumudi.com/news/news.php?id=1527933&u=poster-campaign-against-kpcc-president-change\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_kerala_kaumudi():\n",
    "    # Configuration\n",
    "    base_url = \"https://keralakaumudi.com/news/inc/load-more-latest.php\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    }\n",
    "    days_back = 15\n",
    "    articles_per_day = 5\n",
    "    max_requests = 100  # Safety limit to prevent infinite loops\n",
    "\n",
    "    # Calculate date cutoff (as datetime object)\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    articles = []\n",
    "\n",
    "    offset = 0\n",
    "    requests_made = 0\n",
    "    oldest_date_reached = False\n",
    "\n",
    "    while not oldest_date_reached and requests_made < max_requests:\n",
    "        # Prepare POST request data\n",
    "        data = {\n",
    "            \"offset\": offset,\n",
    "            \"tag\": \"\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Make the request\n",
    "            response = requests.post(base_url, headers=headers, data=data)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML response\n",
    "            soup = BeautifulSoup(response.text.strip(), \"html.parser\")\n",
    "            news_items = soup.find_all(\"div\", class_=\"full-width no-padding cat-news\")\n",
    "            \n",
    "            if not news_items:\n",
    "                break\n",
    "\n",
    "            # Process each news item\n",
    "            for item in news_items:\n",
    "                try:\n",
    "                    # Extract URL\n",
    "                    link = item.find(\"a\", href=True)\n",
    "                    if not link:\n",
    "                        continue\n",
    "                    url = link[\"href\"]\n",
    "\n",
    "                    # Extract title\n",
    "                    title_tag = item.find(\"h5\")\n",
    "                    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "                    # Extract date\n",
    "                    date_tag = item.find(\"span\", class_=\"dt-info\")\n",
    "                    if date_tag:\n",
    "                        date_str = date_tag.get_text(strip=True)\n",
    "                        try:\n",
    "                            article_date = datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "                        except ValueError:\n",
    "                            try:\n",
    "                                article_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "                            except:\n",
    "                                article_date = None\n",
    "                    else:\n",
    "                        article_date = None\n",
    "\n",
    "                    # Skip if we couldn't parse the date\n",
    "                    if not article_date:\n",
    "                        continue\n",
    "\n",
    "                    # Stop if we've reached articles older than our cutoff\n",
    "                    if article_date.date() < cutoff_date.date():\n",
    "                        oldest_date_reached = True\n",
    "                        break\n",
    "\n",
    "                    # Extract summary\n",
    "                    summary_tag = item.find(\"div\", class_=\"full-width no-padding cat-text\").find(\"span\")\n",
    "                    summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
    "\n",
    "                    # Add to our collection\n",
    "                    articles.append({\n",
    "                        \"title\": title,\n",
    "                        \"url\": url,\n",
    "                        \"date\": article_date,\n",
    "                        \"summary\": summary,\n",
    "                        \"datetime\": article_date  # For sorting\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Prepare for next request\n",
    "            offset += len(news_items)\n",
    "            requests_made += 1\n",
    "            time.sleep(1)  # Be polite\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed at offset {offset}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Ensure we have dates in datetime format\n",
    "    if not df.empty:\n",
    "        # Convert all dates to datetime64[ns] if they aren't already\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Filter to only keep articles within our date range\n",
    "        df = df[df['date'] >= pd.to_datetime(cutoff_date)]\n",
    "        \n",
    "        # Sort by date (newest first)\n",
    "        df = df.sort_values('datetime', ascending=False)\n",
    "        \n",
    "        # Get top articles per day\n",
    "        # Convert datetime to date for grouping\n",
    "        df['date_only'] = df['date'].dt.date\n",
    "        top_articles = df.groupby('date_only').head(articles_per_day)\n",
    "        \n",
    "        # Clean up before returning\n",
    "        top_articles = top_articles.drop(columns=['date_only'])\n",
    "        return top_articles.sort_values('date', ascending=False)\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scraping Kerala Kaumudi news...\")\n",
    "    news_df = scrape_kerala_kaumudi()\n",
    "    \n",
    "    if not news_df.empty:\n",
    "        filename = f\"kerala_kaumudi_top_5_per_day_last_15_days_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "        # Convert datetime to string for CSV output\n",
    "        news_df['date'] = news_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        news_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSaved {len(news_df)} articles to {filename}\")\n",
    "        \n",
    "        print(\"\\nSample of most recent articles:\")\n",
    "        print(news_df[['date', 'title', 'url']].head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo recent articles found to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67413d5-a0b5-4040-a989-8af60de04617",
   "metadata": {},
   "source": [
    "## Manorama News Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5647bef7-8332-42eb-b7b9-db24b1cfb0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Manorama Online news...\n",
      "\n",
      "Saved 75 unique articles to manorama_top_5_per_day_last_15_days_20250505.csv\n",
      "\n",
      "Sample of most recent articles:\n",
      "      date                                                                                                                        title                                                                                                                                           url\n",
      "2025-05-05                    തുർക്കി യുദ്ധക്കപ്പൽ കറാച്ചി തുറമുഖത്ത്, ബുധനാഴ്ച വരെ തുടരും; പാക്ക് നാവികസേന ഉദ്യോഗസ്ഥരുമായി കൂടിക്കാഴ്ച                       https://www.manoramaonline.com/news/latest-news/2025/05/05/turkey-pakistan-naval-cooperation-warship-karachi-visit.html\n",
      "2025-05-05                                        അടിവയറ്റിലെ കൊഴുപ്പു നീക്കാൻ ശസ്ത്രക്രിയ; വനിതാ സോഫ്റ്റ്‌വെയർ എൻജിനീയർ ഗുരുതരാവസ്ഥയിൽ                      https://www.manoramaonline.com/district-news/thiruvananthapuram/2025/05/05/software-engineer-critical-after-surgery.html\n",
      "2025-05-05                                നീറ്റ് പരീക്ഷയുടെ ഹാൾ ടിക്കറ്റിൽ പേര് തെറ്റ്, അവസാനം അമ്മയെത്തി പ്രശ്നം ‘നീറ്റായി’ പരിഹരിച്ചു                                      https://www.manoramaonline.com/district-news/idukki/2025/05/05/neet-exam-hall-ticket-mistake-solved.html\n",
      "2025-05-05                            ‘നിങ്ങൾ ആഗ്രഹിക്കുന്നത് തീർച്ചയായും സംഭവിക്കും; മോദിയുടെ പ്രവർത്തനശൈലി ജനങ്ങൾക്ക് നന്നായി അറിയാം’                     https://www.manoramaonline.com/news/latest-news/2025/05/05/india-pakistan-conflict-retaliation-rajnath-singh-warning.html\n",
      "2025-05-05 പെങ്ങൾക്ക് കൂട്ടായി വന്നു, നാടിന്റെ പ്രിയപ്പെട്ടവനായി; ദുരന്തത്തിലേക്ക് വഴിമാറി സൗഹൃദയാത്ര: കണ്ണീർക്കടലായി നെല്ലിമൂട് ഗ്രാമം                     https://www.manoramaonline.com/district-news/thiruvananthapuram/2025/05/05/nellimoodu-velankanni-pilgrimage-accident.html\n",
      "2025-05-04                                  വേളാങ്കണ്ണിയിലേക്ക് പോയ വാഹനം തിരുവാരൂരിൽ അപകടത്തിൽപ്പെട്ടു; നാല് മലയാളികൾക്ക് ദാരുണാന്ത്യം                                     https://www.manoramaonline.com/news/latest-news/2025/05/04/malayalis-died-in-thiruvarur-bus-accident.html\n",
      "2025-05-04                          തീപിടിത്തത്തിനു പിന്നിൽ ബാറ്ററിയിലെ ഇന്റേണൽ ഷോർട്ടേജ്, കത്തിനശിച്ചത് 34 എണ്ണം: പ്രാഥമിക റിപ്പോർട്ട്                https://www.manoramaonline.com/news/latest-news/2025/05/04/kozhikode-medical-college-fire-caused-by-battery-short-circuit.html\n",
      "2025-05-04                                        ആയുധ നിര്‍മാണശാലകളിലെ ജീവനക്കാരുടെ ദീർഘകാല അവധികൾ റദ്ദാക്കി; 2 മാസത്തേക്ക് നിയന്ത്രണം              https://www.manoramaonline.com/news/latest-news/2025/05/04/india-ordnance-factories-boost-production-cancel-long-term-leave.html\n",
      "2025-05-04      പൂരം തടസപ്പെട്ടപ്പോൾ പലതവണ ഫോണിൽ വിളിച്ചിട്ടും കിട്ടിയില്ല, മോശം ഇടപെടലുണ്ടായി: അജിത് കുമാറിനെതിരെ മന്ത്രി രാജന്റെ മൊഴി https://www.manoramaonline.com/news/latest-news/2025/05/04/thrissur-pooram-controversy-minister-rajan-statement-against-adgp-ajith-kumar.html\n",
      "2025-05-04                                         തമിഴ്നാട്ടിൽ ഡിഎംകെയ്ക്കെതിരെ ‘മെഗാ സഖ്യം’; എടപ്പാടി പളനിസാമി മുഖ്യമന്ത്രി സ്ഥാനാർഥി                          https://www.manoramaonline.com/news/latest-news/2025/05/04/edappadi-palanisamy-aiadmk-bjp-alliance-cm-candidate.html\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import re\n",
    "\n",
    "def scrape_manorama_online():\n",
    "    # Configuration\n",
    "    base_url = \"https://www.manoramaonline.com/news/latest-news.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    days_back = 15\n",
    "    articles_per_day = 5\n",
    "    max_pages = 30  # Safety limit to prevent infinite loops\n",
    "\n",
    "    # Calculate date cutoff\n",
    "    cutoff_date = datetime.now() - timedelta(days=days_back)\n",
    "    articles = []\n",
    "    seen_urls = set()  # To track unique URLs\n",
    "\n",
    "    page = 1\n",
    "    oldest_date_reached = False\n",
    "\n",
    "    while not oldest_date_reached and page <= max_pages:\n",
    "        try:\n",
    "            # Construct URL with pagination\n",
    "            if page == 1:\n",
    "                url = base_url\n",
    "            else:\n",
    "                url = f\"{base_url}?page={page}\"\n",
    "\n",
    "            # Make the request\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML response\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            news_items = soup.find_all('li', class_='cmp-story-list__item')\n",
    "            \n",
    "            if not news_items:\n",
    "                break\n",
    "\n",
    "            # Process each news item\n",
    "            for item in news_items:\n",
    "                try:\n",
    "                    # Extract URL\n",
    "                    link = item.find('a', class_='cmp-story-list__title-link', href=True)\n",
    "                    if not link:\n",
    "                        continue\n",
    "                    \n",
    "                    # Construct full URL and check for duplicates\n",
    "                    partial_url = link['href']\n",
    "                    full_url = \"https://www.manoramaonline.com\" + partial_url\n",
    "                    \n",
    "                    if full_url in seen_urls:\n",
    "                        continue  # Skip duplicate articles\n",
    "                    seen_urls.add(full_url)\n",
    "\n",
    "                    # Extract title\n",
    "                    title_tag = item.find('h2', class_='cmp-story-list__title')\n",
    "                    title = title_tag.get_text(strip=True) if title_tag else \"No Title\"\n",
    "\n",
    "                    # Extract date from URL (format: /news/latest-news/2025/05/05/...)\n",
    "                    date_match = re.search(r'/(\\d{4})/(\\d{2})/(\\d{2})/', partial_url)\n",
    "                    if date_match:\n",
    "                        year, month, day = date_match.groups()\n",
    "                        article_date = datetime(int(year), int(month), int(day))\n",
    "                    else:\n",
    "                        # Fallback to current date if no date in URL\n",
    "                        article_date = datetime.now()\n",
    "\n",
    "                    # Stop if we've reached articles older than our cutoff\n",
    "                    if article_date.date() < cutoff_date.date():\n",
    "                        oldest_date_reached = True\n",
    "                        break\n",
    "\n",
    "                    # Extract summary\n",
    "                    summary_tag = item.find('p', class_='cmp-story-list__dispn')\n",
    "                    summary = summary_tag.get_text(strip=True) if summary_tag else \"\"\n",
    "\n",
    "                    # Add to our collection\n",
    "                    articles.append({\n",
    "                        \"title\": title,\n",
    "                        \"url\": full_url,\n",
    "                        \"date\": article_date,\n",
    "                        \"summary\": summary,\n",
    "                        \"datetime\": article_date  # For sorting\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing article: {e}\")\n",
    "                    continue\n",
    "\n",
    "            # Prepare for next page\n",
    "            page += 1\n",
    "            time.sleep(1)  # Be polite\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Request failed at page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(articles)\n",
    "    \n",
    "    # Ensure we have dates in datetime format\n",
    "    if not df.empty:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Filter to only keep articles within our date range\n",
    "        df = df[df['date'] >= pd.to_datetime(cutoff_date)]\n",
    "        \n",
    "        # Final duplicate check (in case same URL appears on different pages)\n",
    "        df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "        \n",
    "        # Sort by date (newest first)\n",
    "        df = df.sort_values('datetime', ascending=False)\n",
    "        \n",
    "        # Get top articles per day\n",
    "        df['date_only'] = df['date'].dt.date\n",
    "        top_articles = df.groupby('date_only').head(articles_per_day)\n",
    "        \n",
    "        # Clean up before returning\n",
    "        top_articles = top_articles.drop(columns=['date_only'])\n",
    "        return top_articles.sort_values('date', ascending=False)\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Scraping Manorama Online news...\")\n",
    "    news_df = scrape_manorama_online()\n",
    "    \n",
    "    if not news_df.empty:\n",
    "        filename = f\"manorama_top_5_per_day_last_15_days_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "        # Convert datetime to string for CSV output\n",
    "        news_df['date'] = news_df['date'].dt.strftime('%Y-%m-%d')\n",
    "        news_df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nSaved {len(news_df)} unique articles to {filename}\")\n",
    "        \n",
    "        print(\"\\nSample of most recent articles:\")\n",
    "        print(news_df[['date', 'title', 'url']].head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo recent articles found to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72179910-0392-4c7b-a944-d5ffea720c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
